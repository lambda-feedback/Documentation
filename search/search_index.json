{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Lambda Feedback \u00b6 This project began at Imperial College in 2022. The documentation is currently under construction. Lambda Feedback is a homework platform providing automated formative feedback . We are initially targeting STEM subjects in Higher Education , with a particular focus on mathematical subjects (such as mathematical methods or mechanics). The value proposition is given briefly here: https://teachingengineers.wordpress.com/2022/07/18/computers-make-us-human/ A demonstration of the software can be found on the homepage under 'try the demo': https://lambdafeedback.com/","title":"Home"},{"location":"#welcome-to-lambda-feedback","text":"This project began at Imperial College in 2022. The documentation is currently under construction. Lambda Feedback is a homework platform providing automated formative feedback . We are initially targeting STEM subjects in Higher Education , with a particular focus on mathematical subjects (such as mathematical methods or mechanics). The value proposition is given briefly here: https://teachingengineers.wordpress.com/2022/07/18/computers-make-us-human/ A demonstration of the software can be found on the homepage under 'try the demo': https://lambdafeedback.com/","title":"Welcome to Lambda Feedback"},{"location":"developer/","text":"Developer Documentation \u00b6 Welcome to LambdaFeedback's developer docs. Evaluation functions \u00b6 Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created. Check out our Quickstart Guide Response area components \u00b6 Guide to create more System Architecture \u00b6 Technologies Deployment pipelines Hierarchy Future Features \u00b6","title":"Developer Documentation"},{"location":"developer/#developer-documentation","text":"Welcome to LambdaFeedback's developer docs.","title":"Developer Documentation"},{"location":"developer/#evaluation-functions","text":"Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created. Check out our Quickstart Guide","title":"Evaluation functions"},{"location":"developer/#response-area-components","text":"Guide to create more","title":"Response area components"},{"location":"developer/#system-architecture","text":"Technologies Deployment pipelines Hierarchy","title":"System Architecture"},{"location":"developer/#future-features","text":"","title":"Future Features"},{"location":"developer/placeholder/","text":"coming soon \u00b6","title":"Specification"},{"location":"developer/placeholder/#coming-soon","text":"","title":"coming soon"},{"location":"developer/evaluation_functions/","text":"Deployed Evaluation Functions \u00b6 Documentation for each of the functions registered to the LambdaFeedback platform are pulled in this section automatically. This is done using a custom MkDocs plugin EvalDocsLoader . If you can't see any documentation files under this section, please contact an admin.","title":"Deployed Evaluation Functions"},{"location":"developer/evaluation_functions/#deployed-evaluation-functions","text":"Documentation for each of the functions registered to the LambdaFeedback platform are pulled in this section automatically. This is done using a custom MkDocs plugin EvalDocsLoader . If you can't see any documentation files under this section, please contact an admin.","title":"Deployed Evaluation Functions"},{"location":"developer/evaluation_functions/local/","text":"Running and Testing Functions Locally \u00b6 Simple \u00b6 Using Docker \u00b6 This method builds and runs evaluation functions in the same way they are deployed on AWS as Lambda functions. Extending a pre-built and AWS-maintained base python image , the container contains a HTTP client which can be used to locally simulate Lambda execution events. Note that this is different from the simple method proposed, in that it gives access to all the functionality provided by the base layer. This means that commands such as docs and healthcheck can be tested. Install Docker on your machine Navigate to the root directory of your function Build the image. This will pull our base image from Dockerhub, extend it with files specific to your evaluation function and name it eval-tmp . docker image build -t eval-tmp app Spin up a container using the image built in the previous step. docker run --rm -d --name eval-function -p 9000 :8080 eval-tmp You can now simulate requests to the function using any request client (like Insomnia or Postman ). By default, the url you can hit is: http://localhost:9000/2015-03-31/functions/function/invocations Warning When deployed, our Lambda functions are triggered by calls made through an AWS API Gateway . This means that when testing locally, events sent should follow the structure of events triggered by that resource. That is, if you want to simulate what it would be like to make web requests to the deployed function. Specifically, this means structuring requests in the following way: { \"headers\" : { \"command\" : \"eval\" }, \"body\" : { \"response\" : \"a\" , \"answer\" : \"a\" , \"params\" : { \"garlic\" : \"moreish\" } } } The main difference is that headers and body are sent as keys in the main body of the local request. When hitting the deployed function through the API Gateway, the command field would instead be passed in the actual HTTP headers of the request - and the actual request body would only contain the response , answer and params fields. (Optional) The run command specifies the -d flag, which spins up the container in detached mode. If you want to inspect the logs of the function, you can run: docker container logs -f eval-function Tip You will very rarely need this, but you can peek into the running container by opening a shell within it using: docker exec -it eval-function bash Useful Links \u00b6","title":"Testing Functions Locally"},{"location":"developer/evaluation_functions/local/#running-and-testing-functions-locally","text":"","title":"Running and Testing Functions Locally"},{"location":"developer/evaluation_functions/local/#simple","text":"","title":"Simple"},{"location":"developer/evaluation_functions/local/#using-docker","text":"This method builds and runs evaluation functions in the same way they are deployed on AWS as Lambda functions. Extending a pre-built and AWS-maintained base python image , the container contains a HTTP client which can be used to locally simulate Lambda execution events. Note that this is different from the simple method proposed, in that it gives access to all the functionality provided by the base layer. This means that commands such as docs and healthcheck can be tested. Install Docker on your machine Navigate to the root directory of your function Build the image. This will pull our base image from Dockerhub, extend it with files specific to your evaluation function and name it eval-tmp . docker image build -t eval-tmp app Spin up a container using the image built in the previous step. docker run --rm -d --name eval-function -p 9000 :8080 eval-tmp You can now simulate requests to the function using any request client (like Insomnia or Postman ). By default, the url you can hit is: http://localhost:9000/2015-03-31/functions/function/invocations Warning When deployed, our Lambda functions are triggered by calls made through an AWS API Gateway . This means that when testing locally, events sent should follow the structure of events triggered by that resource. That is, if you want to simulate what it would be like to make web requests to the deployed function. Specifically, this means structuring requests in the following way: { \"headers\" : { \"command\" : \"eval\" }, \"body\" : { \"response\" : \"a\" , \"answer\" : \"a\" , \"params\" : { \"garlic\" : \"moreish\" } } } The main difference is that headers and body are sent as keys in the main body of the local request. When hitting the deployed function through the API Gateway, the command field would instead be passed in the actual HTTP headers of the request - and the actual request body would only contain the response , answer and params fields. (Optional) The run command specifies the -d flag, which spins up the container in detached mode. If you want to inspect the logs of the function, you can run: docker container logs -f eval-function Tip You will very rarely need this, but you can peek into the running container by opening a shell within it using: docker exec -it eval-function bash","title":"Using Docker "},{"location":"developer/evaluation_functions/local/#useful-links","text":"","title":"Useful Links"},{"location":"developer/evaluation_functions/module/","text":"evaluation-function-utils Package \u00b6 Error Reporting Schema validation Local testing Errors \u00b6 Submodule containing custom error and exception classes, which can be properly caught by the base evaluation layer, and return more detailed and appropriate errors. class EvaluationException \u00b6","title":"Evaluation Function Utils"},{"location":"developer/evaluation_functions/module/#evaluation-function-utils-package","text":"Error Reporting Schema validation Local testing","title":"evaluation-function-utils Package"},{"location":"developer/evaluation_functions/module/#errors","text":"Submodule containing custom error and exception classes, which can be properly caught by the base evaluation layer, and return more detailed and appropriate errors.","title":"Errors"},{"location":"developer/evaluation_functions/module/#class-evaluationexception","text":"","title":"class EvaluationException"},{"location":"developer/evaluation_functions/quickstart/","text":"Developing Evaluation Functions: Getting Started \u00b6 What is an Evaluation Function? \u00b6 It's a cloud function which performs some computation given some user input (the response ), a problem-specific source of truth (the answer ), and some optional parameters ( params ). Evaluation functions capture and automate the role of a teacher who has to keep marking the same question countless times. The simplest example for this would be one which checks for exact equivalence - where the function signals a response is correct only if it is identical to the answer . However, more complex and exotic ones such as symbolic expression equivalence and parsing of physical units can be imagined. Getting Setup for Development \u00b6 Get the code on your local machine (Using github desktop or the git cli) For new functions: create and clone a new repository using the boilerplate template For existing functions: please make your changes on a new separate branch If you are creating a new function , you'll need to set it's name (as it will be deployed) in the config.json file, available in the root directory. The name must be unique. To view existing grading functions, go to: Staging API Gateway Integrations Production API Gateway Integrations You are now ready to start making changes and implementing features by editing each of the three main function-logic files: app/evaluation.py : This file contains the main evaluation_function function, which ultimately gets called to compare a response to an answer . evaluation.py Specification app/evaluation_tests.py : This is where you can test the logic in evaluation.py , following the standard unittest format. evaluation_tests.py Specification app/docs.md : This file should be edited to reflect any changes/features implemented. It is baked into the function's image to be pulled by this documentation website under the deployed functions section. Changes can be tested locally by running the tests you've written using: python -m unittest app/evaluation_tests.py Running and Testing Functions Locally Merge commits into the default branch will trigger the test-and-deploy.yml workflow, which will build the docker image, push it to a shared ECR repository, then call the backend grading-function/ensure route to build the necessary infrastructure to make the function available from the client app. More Info \u00b6 General Function Specification and Behaviour Function philosophy including deployment strategy Request/Response schemas and communication spec Base layer logic, properties and behaviour EvaluationFunctionUtils (python package) Error Reporting Schema validation Local testing","title":"Quickstart Guide"},{"location":"developer/evaluation_functions/quickstart/#developing-evaluation-functions-getting-started","text":"","title":"Developing Evaluation Functions: Getting Started"},{"location":"developer/evaluation_functions/quickstart/#what-is-an-evaluation-function","text":"It's a cloud function which performs some computation given some user input (the response ), a problem-specific source of truth (the answer ), and some optional parameters ( params ). Evaluation functions capture and automate the role of a teacher who has to keep marking the same question countless times. The simplest example for this would be one which checks for exact equivalence - where the function signals a response is correct only if it is identical to the answer . However, more complex and exotic ones such as symbolic expression equivalence and parsing of physical units can be imagined.","title":"What is an Evaluation Function?"},{"location":"developer/evaluation_functions/quickstart/#getting-setup-for-development","text":"Get the code on your local machine (Using github desktop or the git cli) For new functions: create and clone a new repository using the boilerplate template For existing functions: please make your changes on a new separate branch If you are creating a new function , you'll need to set it's name (as it will be deployed) in the config.json file, available in the root directory. The name must be unique. To view existing grading functions, go to: Staging API Gateway Integrations Production API Gateway Integrations You are now ready to start making changes and implementing features by editing each of the three main function-logic files: app/evaluation.py : This file contains the main evaluation_function function, which ultimately gets called to compare a response to an answer . evaluation.py Specification app/evaluation_tests.py : This is where you can test the logic in evaluation.py , following the standard unittest format. evaluation_tests.py Specification app/docs.md : This file should be edited to reflect any changes/features implemented. It is baked into the function's image to be pulled by this documentation website under the deployed functions section. Changes can be tested locally by running the tests you've written using: python -m unittest app/evaluation_tests.py Running and Testing Functions Locally Merge commits into the default branch will trigger the test-and-deploy.yml workflow, which will build the docker image, push it to a shared ECR repository, then call the backend grading-function/ensure route to build the necessary infrastructure to make the function available from the client app.","title":"Getting Setup for Development"},{"location":"developer/evaluation_functions/quickstart/#more-info","text":"General Function Specification and Behaviour Function philosophy including deployment strategy Request/Response schemas and communication spec Base layer logic, properties and behaviour EvaluationFunctionUtils (python package) Error Reporting Schema validation Local testing","title":"More Info"},{"location":"developer/evaluation_functions/specification/","text":"Evaluation Function Specification \u00b6 Philosophy Base Layer \u00b6 File Structures \u00b6 A standard evaluation function repository based on the provided boilerplate will have the following file structure: app/ __init__.py evaluation.py # Script containing the main evaluation_function docs.md # Documentation page for this function (required) evaluation_tests.py # Unittests for the main evaluation_function requirements.txt # list of packages needed for algorithm.py Dockerfile # for building whole image to deploy to AWS .github/ workflows/ test-and-deploy.yml # Testing and deployment pipeline config.json # Specify the name of the evaluation function in this file .gitignore evaluation.py \u00b6 The entire framework, validation and testing developed around evaluation functions is ultimately used to get to this file, or the evaluation_function function within it, to be more precise. The evaluation_function \u00b6 Inputs \u00b6 response : Data input by the user answer : Data to compare user input to (could be from a DB of answers, or pre-generated by other functions) params : Parameters which affect the comparison process (replacements, tolerances, feedbacks, ...) Outputs \u00b6 The function should output a single JSON-encodable dictionary. Although a large amount of freedom is given to what this dict contains, when utilising the function alongside the lambdafeedback web app, a few values are expected/able to be consumed: is_correct: <bool> : Boolean parameter indicate whether the comparison between response and answer was deemed correct under the parameters. This field is then used by the web app to provide the most simple feedback to the user (green/red). Info More standardised function outputs that the frontend can consume are to come Error Handling \u00b6 Error reporting should follow a specific approach for all evaluation functions. If the evaluation_function you've written doesn't throw any errors, then it's output is returned under the result field - and assumed to have worked properly . This means that if you catch an error in your code manually, and simply return it - the frontend will assume everything went fine. Instead, errors can be handled in two ways: Letting evaluation_function fail : On the backend, the call to evaluation_function is wrapped in a try/except which catches any exception. This causes the evaluation to stop completely, returning a standard message, and a repr of the exception thrown in the error.detail field. Custom errors : If you want to report more detailed errors from your function, use the EvaluationException class provided in the evaluation-function-utils package. These are caught before all other standard exceptions, and are dealt with in a different way. These provide a way for your function to throw errors and stop executing safely, while supplying more accurate feedback to the front-end. Example It is discouraged to do the following in the evaluation code: if something . bad . happened (): return { \"error\" : { \"message\" : \"Some important message\" , \"other\" : \"details\" , } } As this causes the actual function output (by the AWS lambda function) to be: { \"command\" : \"eval\" , \"result\" : { \"error\" : { \"message\" : \"Some important message\" , \"other\" : \"details\" } } } Instead, use custom exceptions from the evaluation-function-utils package. if something . bad . happened (): raise EvaluationException ( message = \"Some important message\" , other = 'details' ) As the actual function output will look like: { \"command\" : \"eval\" , \"error\" : { \"message\" : \"Some important message\" , \"other\" : \"details\" } } This immediately indicates to the frontend client that something has gone wrong, allowing for proper feedback to be displayed. evaluation_tests.py \u00b6 docs.md \u00b6","title":"General Specification"},{"location":"developer/evaluation_functions/specification/#evaluation-function-specification","text":"Philosophy","title":"Evaluation Function Specification"},{"location":"developer/evaluation_functions/specification/#base-layer","text":"","title":"Base Layer"},{"location":"developer/evaluation_functions/specification/#file-structures","text":"A standard evaluation function repository based on the provided boilerplate will have the following file structure: app/ __init__.py evaluation.py # Script containing the main evaluation_function docs.md # Documentation page for this function (required) evaluation_tests.py # Unittests for the main evaluation_function requirements.txt # list of packages needed for algorithm.py Dockerfile # for building whole image to deploy to AWS .github/ workflows/ test-and-deploy.yml # Testing and deployment pipeline config.json # Specify the name of the evaluation function in this file .gitignore","title":"File Structures"},{"location":"developer/evaluation_functions/specification/#evaluationpy","text":"The entire framework, validation and testing developed around evaluation functions is ultimately used to get to this file, or the evaluation_function function within it, to be more precise.","title":"evaluation.py"},{"location":"developer/evaluation_functions/specification/#the-evaluation_function","text":"","title":"The evaluation_function"},{"location":"developer/evaluation_functions/specification/#inputs","text":"response : Data input by the user answer : Data to compare user input to (could be from a DB of answers, or pre-generated by other functions) params : Parameters which affect the comparison process (replacements, tolerances, feedbacks, ...)","title":"Inputs"},{"location":"developer/evaluation_functions/specification/#outputs","text":"The function should output a single JSON-encodable dictionary. Although a large amount of freedom is given to what this dict contains, when utilising the function alongside the lambdafeedback web app, a few values are expected/able to be consumed: is_correct: <bool> : Boolean parameter indicate whether the comparison between response and answer was deemed correct under the parameters. This field is then used by the web app to provide the most simple feedback to the user (green/red). Info More standardised function outputs that the frontend can consume are to come","title":"Outputs"},{"location":"developer/evaluation_functions/specification/#error-handling","text":"Error reporting should follow a specific approach for all evaluation functions. If the evaluation_function you've written doesn't throw any errors, then it's output is returned under the result field - and assumed to have worked properly . This means that if you catch an error in your code manually, and simply return it - the frontend will assume everything went fine. Instead, errors can be handled in two ways: Letting evaluation_function fail : On the backend, the call to evaluation_function is wrapped in a try/except which catches any exception. This causes the evaluation to stop completely, returning a standard message, and a repr of the exception thrown in the error.detail field. Custom errors : If you want to report more detailed errors from your function, use the EvaluationException class provided in the evaluation-function-utils package. These are caught before all other standard exceptions, and are dealt with in a different way. These provide a way for your function to throw errors and stop executing safely, while supplying more accurate feedback to the front-end. Example It is discouraged to do the following in the evaluation code: if something . bad . happened (): return { \"error\" : { \"message\" : \"Some important message\" , \"other\" : \"details\" , } } As this causes the actual function output (by the AWS lambda function) to be: { \"command\" : \"eval\" , \"result\" : { \"error\" : { \"message\" : \"Some important message\" , \"other\" : \"details\" } } } Instead, use custom exceptions from the evaluation-function-utils package. if something . bad . happened (): raise EvaluationException ( message = \"Some important message\" , other = 'details' ) As the actual function output will look like: { \"command\" : \"eval\" , \"error\" : { \"message\" : \"Some important message\" , \"other\" : \"details\" } } This immediately indicates to the frontend client that something has gone wrong, allowing for proper feedback to be displayed.","title":"Error Handling"},{"location":"developer/evaluation_functions/specification/#evaluation_testspy","text":"","title":"evaluation_tests.py"},{"location":"developer/evaluation_functions/specification/#docsmd","text":"","title":"docs.md"},{"location":"intro/","text":"Introduction \u00b6 General site information Terminology used Definitions","title":"Introduction"},{"location":"intro/#introduction","text":"General site information Terminology used Definitions","title":"Introduction"},{"location":"intro/contacts/","text":"Contacts \u00b6 Who to contact","title":"Contacts"},{"location":"intro/contacts/#contacts","text":"Who to contact","title":"Contacts"},{"location":"student/","text":"Student/User Documentation \u00b6 Quickstart guides Student dashboard help Response area information (how to input equations, etc...) (This could also just be a link to the guide used by teachers)","title":"Student/User Documentation"},{"location":"student/#studentuser-documentation","text":"Quickstart guides Student dashboard help Response area information (how to input equations, etc...) (This could also just be a link to the guide used by teachers)","title":"Student/User Documentation"},{"location":"teacher/","text":"Teacher and Content Author Documentation \u00b6 Guides/Specs for how to author content on the LambdaFeedback Platform Content Structure Content Blocks Response Areas Evaluation Functions","title":"Teacher and Content Author Documentation"},{"location":"teacher/#teacher-and-content-author-documentation","text":"Guides/Specs for how to author content on the LambdaFeedback Platform Content Structure Content Blocks Response Areas Evaluation Functions","title":"Teacher and Content Author Documentation"},{"location":"teacher/guides/analytics/","text":"Analytics Dashboard Guide \u00b6","title":"Analytics"},{"location":"teacher/guides/analytics/#analytics-dashboard-guide","text":"","title":"Analytics Dashboard Guide"},{"location":"teacher/guides/content/","text":"Content Creation Guide \u00b6","title":"Content Creation"},{"location":"teacher/guides/content/#content-creation-guide","text":"","title":"Content Creation Guide"},{"location":"teacher/guides/getting_started/","text":"Getting Started \u00b6 This could be the default page for teacher guides, which links to more detailed ones at the bottom Information about all the pages available to the Teacher - Analytics dashboard - Content editing - Student mode","title":"Getting Started"},{"location":"teacher/guides/getting_started/#getting-started","text":"This could be the default page for teacher guides, which links to more detailed ones at the bottom Information about all the pages available to the Teacher - Analytics dashboard - Content editing - Student mode","title":"Getting Started"},{"location":"teacher/reference/evaluation_functions/","text":"Evaluation Functions \u00b6 Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created.","title":"Evaluation Functions"},{"location":"teacher/reference/evaluation_functions/#evaluation-functions","text":"Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created.","title":"Evaluation Functions"},{"location":"teacher/reference/response_area_components/","text":"Response Area Components \u00b6 These are what the user interacts with on the front-end. As React components, they admit a certain number of parameters which are described in this section. Their basic operation is shown in the following gifs ExpressionInput Response Component, with symbolicEqual grading function \u00b6 NumericUnits Response Component, with wolframAlphaEqual grading function \u00b6","title":"Response Area Components"},{"location":"teacher/reference/response_area_components/#response-area-components","text":"These are what the user interacts with on the front-end. As React components, they admit a certain number of parameters which are described in this section. Their basic operation is shown in the following gifs","title":"Response Area Components"},{"location":"teacher/reference/response_area_components/#expressioninput-response-component-with-symbolicequal-grading-function","text":"","title":"ExpressionInput Response Component, with symbolicEqual grading function"},{"location":"teacher/reference/response_area_components/#numericunits-response-component-with-wolframalphaequal-grading-function","text":"","title":"NumericUnits Response Component, with wolframAlphaEqual grading function"},{"location":"teacher/reference/response_area_components/ExpressionInput/","text":"ExpressionInput \u00b6 This response area is very similar to TextInput , differing in that it can display how the user's response was interpreted back to them. This is done by the grading function providing a feedback.response_latex field, which gets rendered. Component Parameters \u00b6 post_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is simply sent as a string, exactly how the user input it in the input field. Example \"response\" : \"4*x + 1\" Example Screenshot \u00b6","title":"ExpressionInput"},{"location":"teacher/reference/response_area_components/ExpressionInput/#expressioninput","text":"This response area is very similar to TextInput , differing in that it can display how the user's response was interpreted back to them. This is done by the grading function providing a feedback.response_latex field, which gets rendered.","title":"ExpressionInput"},{"location":"teacher/reference/response_area_components/ExpressionInput/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/ExpressionInput/#post_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"post_response_text (optional)"},{"location":"teacher/reference/response_area_components/ExpressionInput/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/ExpressionInput/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is simply sent as a string, exactly how the user input it in the input field. Example \"response\" : \"4*x + 1\"","title":"Response Structure"},{"location":"teacher/reference/response_area_components/ExpressionInput/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"teacher/reference/response_area_components/Matrix/","text":"Matrix \u00b6 Matrix response area. Will populate the component with a grid of text input fields, in order to facilitate inputing matrices. Component Parameters \u00b6 shape (required) \u00b6 Required paramter, describes the shape of the Matrix to be displayed. This should be given as a length 2 array. Example \"shape\" : [ 2 , 2 ] post_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The user reponses are gathered into a nested list, and parsed using the JavaScript parseFloat function. Example \"response\" : [ [ 123.0 , 2.0 ], [ 8.0 , 10.0 ] ] Example Screenshot \u00b6","title":"Matrix"},{"location":"teacher/reference/response_area_components/Matrix/#matrix","text":"Matrix response area. Will populate the component with a grid of text input fields, in order to facilitate inputing matrices.","title":"Matrix"},{"location":"teacher/reference/response_area_components/Matrix/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/Matrix/#shape-required","text":"Required paramter, describes the shape of the Matrix to be displayed. This should be given as a length 2 array. Example \"shape\" : [ 2 , 2 ]","title":"shape (required)"},{"location":"teacher/reference/response_area_components/Matrix/#post_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"post_response_text (optional)"},{"location":"teacher/reference/response_area_components/Matrix/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/Matrix/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The user reponses are gathered into a nested list, and parsed using the JavaScript parseFloat function. Example \"response\" : [ [ 123.0 , 2.0 ], [ 8.0 , 10.0 ] ]","title":"Response Structure"},{"location":"teacher/reference/response_area_components/Matrix/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"teacher/reference/response_area_components/MultipleChoice/","text":"MultipleChoice \u00b6 General multiple choice response area. Features multiple options for single answer and randomising the order. Parameters \u00b6 options (required) \u00b6 This is an array containing strings, each representing an option in the multiple choice component. These are parsed using the parseEquations function, meaning they can support markdown styling and LaTeX. Example \"options\" : [ \"\\\\( 4x^2 + 2 = \\\\frac{\\\\delta y}{\\\\delta x} \\\\)\" , \"\\\\( \\\\pi = 3 \\\\)\" , \"\\\\( K_{iakb} U^{b}_{k} = f^{a}_{i} \\\\)\" , \"\\\\( 3 = \\\\pi \\\\)\" , ] shuffle (optional) \u00b6 This is an optional boolean which will shuffle the options array on each render of this component. singleAnswer (optional) \u00b6 By default, each item options is rendered using the html checkbox input type. Setting the singleAnswer boolean flag will turn those into radio buttons. Note Changing this flag will alter the shape of the Response Structure , and potentially require changing the grading function type and settings. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. This structure is different depending on if the singleAnswer option was used: singleAnswer == False (or undefined) \u00b6 In this case, the user data is saved as an array with the same length as options , where each item is either a 1 or a 0 depending on if the corresponding option was selected. Example If for an instance where there are 4 options, and the first and third options were selected, the response field would be: \"response\" : [ 1 , 0 , 1 , 0 ] singleAnswer == True \u00b6 In this case, there is only one correct answer, and each option is displayed as a radio button. Therefore the response field contains only one integer, corresponding to the index of the selected option. Example If for an instance where there are 4 options, and the third option was selected, the response field would be: \"response\" : 2 Example Screenshot \u00b6 This shows a response where singleAnswer was set to False, since each option is displayed as a checkbox","title":"MultipleChoice"},{"location":"teacher/reference/response_area_components/MultipleChoice/#multiplechoice","text":"General multiple choice response area. Features multiple options for single answer and randomising the order.","title":"MultipleChoice"},{"location":"teacher/reference/response_area_components/MultipleChoice/#parameters","text":"","title":"Parameters"},{"location":"teacher/reference/response_area_components/MultipleChoice/#options-required","text":"This is an array containing strings, each representing an option in the multiple choice component. These are parsed using the parseEquations function, meaning they can support markdown styling and LaTeX. Example \"options\" : [ \"\\\\( 4x^2 + 2 = \\\\frac{\\\\delta y}{\\\\delta x} \\\\)\" , \"\\\\( \\\\pi = 3 \\\\)\" , \"\\\\( K_{iakb} U^{b}_{k} = f^{a}_{i} \\\\)\" , \"\\\\( 3 = \\\\pi \\\\)\" , ]","title":"options (required)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#shuffle-optional","text":"This is an optional boolean which will shuffle the options array on each render of this component.","title":"shuffle (optional)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-optional","text":"By default, each item options is rendered using the html checkbox input type. Setting the singleAnswer boolean flag will turn those into radio buttons. Note Changing this flag will alter the shape of the Response Structure , and potentially require changing the grading function type and settings.","title":"singleAnswer (optional)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. This structure is different depending on if the singleAnswer option was used:","title":"Response Structure"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-false-or-undefined","text":"In this case, the user data is saved as an array with the same length as options , where each item is either a 1 or a 0 depending on if the corresponding option was selected. Example If for an instance where there are 4 options, and the first and third options were selected, the response field would be: \"response\" : [ 1 , 0 , 1 , 0 ]","title":"singleAnswer == False (or undefined)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-true","text":"In this case, there is only one correct answer, and each option is displayed as a radio button. Therefore the response field contains only one integer, corresponding to the index of the selected option. Example If for an instance where there are 4 options, and the third option was selected, the response field would be: \"response\" : 2","title":"singleAnswer == True"},{"location":"teacher/reference/response_area_components/MultipleChoice/#example-screenshot","text":"This shows a response where singleAnswer was set to False, since each option is displayed as a checkbox","title":"Example Screenshot"},{"location":"teacher/reference/response_area_components/NumberInput/","text":"NumberInput \u00b6 Very similar to the TextInput response area, except the user response is parsed as a float. Component Parameters \u00b6 post_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is simply sent as a float, parsed from the input field using the JavaScript parseFloat function. Example \"response\" : 15.8 Example Screenshot \u00b6","title":"NumberInput"},{"location":"teacher/reference/response_area_components/NumberInput/#numberinput","text":"Very similar to the TextInput response area, except the user response is parsed as a float.","title":"NumberInput"},{"location":"teacher/reference/response_area_components/NumberInput/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/NumberInput/#post_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"post_response_text (optional)"},{"location":"teacher/reference/response_area_components/NumberInput/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/NumberInput/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is simply sent as a float, parsed from the input field using the JavaScript parseFloat function. Example \"response\" : 15.8","title":"Response Structure"},{"location":"teacher/reference/response_area_components/NumberInput/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"teacher/reference/response_area_components/NumericUnits/","text":"NumericUnits \u00b6 Provides two input fields with Number and Units placeholder texts. This area will also display its associated grading function (as seen in the screenshot below). Note this area will display how the user's response was interpred using the interp_string field provided in the feedback object returned by that function (if it exists). Component Parameters \u00b6 pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. In this case, the response is a single string which features the user's response to both fields separated by a space. Example \"response\" : \"150 g\" Example Screenshot \u00b6","title":"NumericUnits"},{"location":"teacher/reference/response_area_components/NumericUnits/#numericunits","text":"Provides two input fields with Number and Units placeholder texts. This area will also display its associated grading function (as seen in the screenshot below). Note this area will display how the user's response was interpred using the interp_string field provided in the feedback object returned by that function (if it exists).","title":"NumericUnits"},{"location":"teacher/reference/response_area_components/NumericUnits/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/NumericUnits/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/NumericUnits/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. In this case, the response is a single string which features the user's response to both fields separated by a space. Example \"response\" : \"150 g\"","title":"Response Structure"},{"location":"teacher/reference/response_area_components/NumericUnits/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"teacher/reference/response_area_components/TextInput/","text":"TextInput \u00b6 Most basic response area, displays a single input field, accepting and text from the user. Component Parameters \u00b6 pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is a string containing the user's response, exactly as they typed it in the input field. Example \"response\" : \"incompressible\" Example Screenshot \u00b6","title":"TextInput"},{"location":"teacher/reference/response_area_components/TextInput/#textinput","text":"Most basic response area, displays a single input field, accepting and text from the user.","title":"TextInput"},{"location":"teacher/reference/response_area_components/TextInput/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/TextInput/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/TextInput/#pre_response_text-optional_1","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/TextInput/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is a string containing the user's response, exactly as they typed it in the input field. Example \"response\" : \"incompressible\"","title":"Response Structure"},{"location":"teacher/reference/response_area_components/TextInput/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"teacher/reference/response_area_components/TrueFalse/","text":"TrueFalse \u00b6 Simple True or False Response Area component. Shows two options \"True\" and \"False\" selectable by radio buttons. Component Parameters \u00b6 none Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is an integer equal to 1 if the true option was selected, and 0 if the false option was selected. Example \"response\" : 1 Example Screenshot \u00b6","title":"TrueFalse"},{"location":"teacher/reference/response_area_components/TrueFalse/#truefalse","text":"Simple True or False Response Area component. Shows two options \"True\" and \"False\" selectable by radio buttons.","title":"TrueFalse"},{"location":"teacher/reference/response_area_components/TrueFalse/#component-parameters","text":"none","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/TrueFalse/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is an integer equal to 1 if the true option was selected, and 0 if the false option was selected. Example \"response\" : 1","title":"Response Structure"},{"location":"teacher/reference/response_area_components/TrueFalse/#example-screenshot","text":"","title":"Example Screenshot"}]}